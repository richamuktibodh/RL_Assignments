{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 8.8 4.4 5.3 1.5 \n",
      "1.5 3.0 2.3 1.9 0.5 \n",
      "0.1 0.7 0.7 0.4 -0.4 \n",
      "-1.0 -0.4 -0.4 -0.6 -1.2 \n",
      "-1.9 -1.3 -1.2 -1.4 -2.0 \n"
     ]
    }
   ],
   "source": [
    "def generate_reward(i, j, action):\n",
    "    if i == 0 and j == 1: # for A\n",
    "        return 10\n",
    "    elif i == 0 and j == 3: # for B\n",
    "        return 5\n",
    "    elif action == 'up'and i == 0:\n",
    "        return -1\n",
    "    elif action == 'down' and i == 4:\n",
    "        return -1\n",
    "    elif action == 'left' and j == 0:\n",
    "        return -1\n",
    "    elif action == 'right' and j == 4:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# grid for value functions is maze, each block is a state.\n",
    "maze = [[0,0,0,0,0] for i in range(5)]\n",
    "gamma = 0.9\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "for k in range(2000):\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            temp = 0\n",
    "            for action in actions:\n",
    "                reward = generate_reward(i, j, action)\n",
    "                # if reward is -1, then the s' is invalid, so we don't update it using value function of s'.\n",
    "                if reward != -1:\n",
    "                    if i == 0 and j == 1: # for A\n",
    "                        temp +=  (reward + gamma * maze[4][1])/4\n",
    "                    elif i == 0 and j == 3: # for B\n",
    "                        temp +=  (reward + gamma * maze[2][3])/4\n",
    "                    elif action == 'up':\n",
    "                        temp +=  (reward + gamma * maze[i-1][j])/4\n",
    "                    elif action == 'down':\n",
    "                        temp +=  (reward + gamma * maze[i+1][j])/4\n",
    "                    elif action == 'left':\n",
    "                        temp +=  (reward + gamma * maze[i][j-1])/4\n",
    "                    elif action == 'right':\n",
    "                        temp +=  (reward + gamma * maze[i][j+1])/4\n",
    "                else:\n",
    "                    temp += (reward + gamma * maze[i][j])/4\n",
    "            maze[i][j] = temp # updating the value function\n",
    "                    \n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(round(maze[i][j], 1), end = ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function:\n",
      "22.0 24.4 22.0 19.4 17.5 \n",
      "19.8 22.0 19.8 17.8 16.0 \n",
      "17.8 19.8 17.8 16.0 14.4 \n",
      "16.0 17.8 16.0 14.4 13.0 \n",
      "14.4 16.0 14.4 13.0 11.7 \n"
     ]
    }
   ],
   "source": [
    "# grid for optimal value functions is maze, each block is a state.\n",
    "maze = [[0,0,0,0,0] for i in range(5)]\n",
    "gamma = 0.9\n",
    "# actions = ['up', 'down', 'left', 'right']\n",
    "# map each action to a number\n",
    "action_map = {'up':0, 'down':1, 'left':2, 'right':3}\n",
    "for k in range(2000):\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            temp = 0\n",
    "            for action in action_map.keys():\n",
    "                reward = generate_reward(i, j, action)\n",
    "                # if reward is -1, then the s' is invalid, so we don't update it using value function of s'.\n",
    "                if reward != -1:\n",
    "                    if i == 0 and j == 1: # for A\n",
    "                        temp = max(temp, reward + gamma * maze[4][1])\n",
    "                    elif i == 0 and j == 3: # for B\n",
    "                        temp = max(temp, reward + gamma * maze[2][3])\n",
    "                    elif action == 'up':\n",
    "                        temp = max(temp, reward + gamma * maze[i-1][j])\n",
    "                    elif action == 'down':\n",
    "                        temp = max(temp, reward + gamma * maze[i+1][j])\n",
    "                    elif action == 'left':\n",
    "                        temp = max(temp, reward + gamma * maze[i][j-1])\n",
    "                    elif action == 'right':\n",
    "                        temp = max(temp, reward + gamma * maze[i][j+1])\n",
    "                else:\n",
    "                    temp = max(temp, reward + gamma * maze[i][j]) # updating the value function using s itself bcz s' is invalid\n",
    "            maze[i][j] = temp\n",
    "            \n",
    "print(\"Optimal value function:\")\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(round(maze[i][j], 1), end = ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\n",
      "[0, 0, 0, 1] [1, 1, 1, 1] [0, 0, 1, 0] [1, 1, 1, 1] [0, 0, 1, 0] \n",
      "[1, 0, 0, 1] [1, 0, 0, 0] [1, 0, 1, 0] [0, 0, 1, 0] [0, 0, 1, 0] \n",
      "[1, 0, 0, 1] [1, 0, 0, 0] [1, 0, 1, 0] [1, 0, 1, 0] [1, 0, 1, 0] \n",
      "[1, 0, 0, 1] [1, 0, 0, 0] [1, 0, 1, 0] [1, 0, 1, 0] [1, 0, 1, 0] \n",
      "[1, 0, 0, 1] [1, 0, 0, 0] [1, 0, 1, 0] [1, 0, 1, 0] [1, 0, 1, 0] \n"
     ]
    }
   ],
   "source": [
    "# make a 3d matrix to store the optimal policy, each block is a state, and each state has 4 actions.\n",
    "opt_action = [[[0,0,0,0] for i in range(5)] for j in range(5)]\n",
    "gamma = 0.9\n",
    "# print(opt_action[0])\n",
    "# print(opt_action[0][0])\n",
    "# for k in range(20):\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        temp_q = 0\n",
    "        action_qvals = [0,0,0,0]\n",
    "        if i == 0 and j == 1: # for A\n",
    "            opt_action[i][j] = [1,1,1,1]\n",
    "            continue\n",
    "        elif i == 0 and j == 3: # for B\n",
    "            opt_action[i][j] = [1,1,1,1]\n",
    "            continue\n",
    "        for action in action_map.keys():\n",
    "            reward = generate_reward(i, j, action)\n",
    "            # if reward is -1, then the s' is invalid, so we don't update it using value function of s'.\n",
    "            # handle A and B later\n",
    "            if reward != -1:\n",
    "                if action == 'up':\n",
    "                    temp_q = reward + gamma * maze[i-1][j]\n",
    "                    action_qvals[0] = temp_q\n",
    "                elif action == 'down':\n",
    "                    temp_q = reward + gamma * maze[i+1][j]\n",
    "                    action_qvals[1] = temp_q\n",
    "                elif action == 'left':\n",
    "                    temp_q = reward + gamma * maze[i][j-1]\n",
    "                    action_qvals[2] = temp_q\n",
    "                elif action == 'right':\n",
    "                    temp_q = reward + gamma * maze[i][j+1]\n",
    "                    action_qvals[3] = temp_q\n",
    "            else:\n",
    "                temp_q = reward + gamma * maze[i][j]\n",
    "                action_qvals[action_map[action]] = temp_q\n",
    "        # take the max q value and update the optimal policy\n",
    "        max_q = max(action_qvals)\n",
    "        # if there are multiple actions with max q value, update all of them to 1\n",
    "        for idx, q in enumerate(action_qvals):\n",
    "            if q == max_q:\n",
    "                opt_action[i][j][idx] = 1\n",
    "\n",
    "# print the optimal policy row by row\n",
    "print(\"Optimal policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\")\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        print(opt_action[i][j], end = ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value function:\n",
      "0 -14.0 -20.0 -22.0 \n",
      "-14.0 -18.0 -20.0 -20.0 \n",
      "-20.0 -20.0 -18.0 -14.0 \n",
      "-22.0 -20.0 -14.0 0 \n",
      "policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\n",
      "[1, 1, 1, 1] [0, 0, 1, 0] [0, 0, 1, 0] [0, 0, 1, 0] \n",
      "[1, 0, 0, 0] [1, 0, 1, 0] [0, 0, 1, 0] [0, 1, 0, 0] \n",
      "[1, 0, 0, 0] [1, 0, 0, 0] [0, 0, 0, 1] [0, 1, 0, 0] \n",
      "[1, 0, 0, 0] [0, 0, 0, 1] [0, 0, 0, 1] [1, 1, 1, 1] \n",
      "\n",
      "value function:\n",
      "0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0 \n",
      "policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\n",
      "[1, 1, 1, 1] [0, 0, 1, 0] [0, 0, 1, 0] [0, 1, 1, 0] \n",
      "[1, 0, 0, 0] [1, 0, 1, 0] [1, 1, 1, 1] [0, 1, 0, 0] \n",
      "[1, 0, 0, 0] [1, 1, 1, 1] [0, 1, 0, 1] [0, 1, 0, 0] \n",
      "[1, 0, 0, 1] [0, 0, 0, 1] [0, 0, 0, 1] [1, 1, 1, 1] \n",
      "\n",
      "value function:\n",
      "0 -1.0 -2.0 -3.0 \n",
      "-1.0 -2.0 -3.0 -2.0 \n",
      "-2.0 -3.0 -2.0 -1.0 \n",
      "-3.0 -2.0 -1.0 0 \n",
      "policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\n",
      "[1, 1, 1, 1] [0, 0, 1, 0] [0, 0, 1, 0] [0, 1, 1, 0] \n",
      "[1, 0, 0, 0] [1, 0, 1, 0] [1, 1, 1, 1] [0, 1, 0, 0] \n",
      "[1, 0, 0, 0] [1, 1, 1, 1] [0, 1, 0, 1] [0, 1, 0, 0] \n",
      "[1, 0, 0, 1] [0, 0, 0, 1] [0, 0, 0, 1] [1, 1, 1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def stateValid(i, j):\n",
    "    if i >= 0 and i <= 3 and j >= 0 and j <= 3:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "# up down left right\n",
    "def policy_eval(grid, action, theta, reward = -1, gamma = 1):\n",
    "    while True:\n",
    "        # print(grid[1][1])\n",
    "        delta = 0\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if i == 0 and j == 0: # terminal state\n",
    "                    continue\n",
    "                elif i == 3 and j == 3: # terminal state\n",
    "                    continue\n",
    "                v = grid[i][j] # store the old value function\n",
    "                temp = 0\n",
    "                count = 0\n",
    "                if action[i][j][0] == 1:\n",
    "                    count += 1\n",
    "                    new_i = i - 1\n",
    "                    new_j = j\n",
    "                    if stateValid(new_i, new_j):\n",
    "                        temp +=  (reward + gamma * grid[new_i][new_j])\n",
    "                    else:\n",
    "                        temp +=  (reward + gamma * grid[i][j])\n",
    "\n",
    "                if action[i][j][1] == 1:\n",
    "                    count += 1\n",
    "                    new_i = i + 1\n",
    "                    new_j = j\n",
    "                    if stateValid(new_i, new_j):\n",
    "                        temp +=  (reward + gamma * grid[new_i][new_j])\n",
    "                    else:\n",
    "                        temp +=  (reward + gamma * grid[i][j])\n",
    "                if action[i][j][2] == 1:\n",
    "                    count += 1\n",
    "                    new_i = i\n",
    "                    new_j = j - 1\n",
    "                    if stateValid(new_i, new_j):\n",
    "                        temp +=  (reward + gamma * grid[new_i][new_j])\n",
    "                    else:\n",
    "                        temp +=  (reward + gamma * grid[i][j])\n",
    "                if action[i][j][3] == 1:\n",
    "                    count += 1\n",
    "                    new_i = i\n",
    "                    new_j = j + 1\n",
    "                    if stateValid(new_i, new_j):\n",
    "                        temp +=  (reward + gamma * grid[new_i][new_j])\n",
    "                    else:\n",
    "                        temp +=  (reward + gamma * grid[i][j])\n",
    "                grid[i][j] = temp/count # update the value function\n",
    "                delta = max(delta, abs(v - grid[i][j]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return grid\n",
    "    \n",
    "def policy_improv(opt_action, maze, reward = -1, gamma = 1):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if i == 0 and j == 0: # terminal state\n",
    "                continue\n",
    "            elif i == 3 and j == 3: # terminal state\n",
    "                continue\n",
    "            temp_q = 0\n",
    "            action_qvals = [0,0,0,0]\n",
    "            for action in action_map.keys():\n",
    "                reward = generate_reward(i, j, action)\n",
    "                # if reward is -1, then the s' is invalid, so we don't update it using value function of s'.\n",
    "                # handle A and B later\n",
    "                if action == 'up':\n",
    "                    if stateValid(i-1, j):\n",
    "                        temp_q = reward + gamma * maze[i-1][j]\n",
    "                        action_qvals[0] = temp_q\n",
    "                    else:\n",
    "                        temp_q = reward + gamma * maze[i][j]\n",
    "                        action_qvals[0] = temp_q\n",
    "                elif action == 'down' :\n",
    "                    if stateValid(i+1, j):\n",
    "                        temp_q = reward + gamma * maze[i+1][j]\n",
    "                        action_qvals[1] = temp_q\n",
    "                    else:\n",
    "                        temp_q = reward + gamma * maze[i][j]\n",
    "                        action_qvals[1] = temp_q\n",
    "                elif action == 'left' :\n",
    "                    if stateValid(i, j-1):\n",
    "                        temp_q = reward + gamma * maze[i][j-1]\n",
    "                        action_qvals[2] = temp_q\n",
    "                    else:\n",
    "                        temp_q = reward + gamma * maze[i][j]\n",
    "                        action_qvals[2] = temp_q\n",
    "                elif action == 'right':\n",
    "                    if stateValid(i, j+1):\n",
    "                        temp_q = reward + gamma * maze[i][j+1]\n",
    "                        action_qvals[3] = temp_q\n",
    "                    else:\n",
    "                        temp_q = reward + gamma * maze[i][j]\n",
    "                        action_qvals[3] = temp_q\n",
    "                        \n",
    "\n",
    "        # take the max q value and update the optimal policy\n",
    "            max_q = max(action_qvals)\n",
    "            # if there are multiple actions with max q value, update all of them to 1\n",
    "            for idx, q in enumerate(action_qvals):\n",
    "                if q == max_q:\n",
    "                    opt_action[i][j][idx] = 1\n",
    "                else:\n",
    "                    opt_action[i][j][idx] = 0\n",
    "    return opt_action\n",
    "\n",
    "\n",
    "\n",
    "reward = -1\n",
    "value_maze = [[0,0,0,0] for i in range(4)]\n",
    "opt_action = [[[1,1,1,1] for i in range(4)] for j in range(4)]\n",
    "gamma = 1\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "theta = 0.0001\n",
    "# print(opt_action[0][0])\n",
    "# for _ in range(3):\n",
    "while True:\n",
    "    old_action = copy.deepcopy(opt_action)\n",
    "    # print(old_action)\n",
    "    value_maze = policy_eval(value_maze, opt_action, theta)\n",
    "    # print(old_action)\n",
    "    opt_action = policy_improv(opt_action, value_maze)\n",
    "    # print(old_action)\n",
    "    print(\"value function:\")\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            print(round(value_maze[i][j], 1), end = ' ')\n",
    "        print()\n",
    "    print(\"policy: format is [up, down, left, right], 1 means optimal action, 0 means not optimal action\")\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            print(opt_action[i][j], end = ' ')\n",
    "        print()\n",
    "    print()\n",
    "    # for i in range(4):\n",
    "    #     for j in range(4):\n",
    "    #         print(old_action[i][j], end = ' ')\n",
    "    #     print()\n",
    "    if old_action == opt_action:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
